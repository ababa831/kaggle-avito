{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN model for title and description data\n",
    "# Bi LSTM + GRU with embedding features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "from collections import Counter\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h5py\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU, Bidirectional\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTEXT_PRETRAINED_NEOLOGD_PATH = './fastText_Avito.vec'\n",
    "FTEXT_PRETRAINED_WIKI_PATH = './wiki.ru.vec'\n",
    "FTEXT_PRETRAINED_COMMONCRAWL_PATH = './cc.ru.300.vec'\n",
    "TOKENIZER_PATH = './tokenizer.pkl'\n",
    "NEW_WORDS_LIST_PATH = './new_wordlist.pkl'\n",
    "MODEL_HISTORY_PATH = './hist.pkl'\n",
    "PRETRAINED_MODEL_PATH = './best_model_no_cross_val.h5'\n",
    "\n",
    "fold_num = 4\n",
    "seed = 7\n",
    "VALID = False\n",
    "\n",
    "seq_maxlen = 300\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanName(text):\n",
    "    try:\n",
    "        textProc = text.lower()\n",
    "        # textProc = \" \".join(map(str.strip, re.split('(\\d+)',textProc)))\n",
    "        #regex = re.compile(u'[^[:alpha:]]')\n",
    "        #textProc = regex.sub(\" \", textProc)\n",
    "        textProc = re.sub('[!@#$_“”¨«»®´·º½¾¿¡§£₤‘’]', '', textProc)\n",
    "        textProc = \" \".join(textProc.split())\n",
    "        return textProc\n",
    "    except: \n",
    "        return \"name error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()\n",
    "    removed_words = [word for word in words if not word in stopwords]\n",
    "    filtered_text =  \" \".join(removed_words)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_train_feature(train_df, seq_maxlen=300, max_num_words=100000):\n",
    "    \"\"\"\n",
    "    コメント(文字列) -> トークン化された特徴, ラベル\n",
    "    \"\"\"\n",
    "    print(\"学習データの語彙数を計算しています\")\n",
    "    total_word_count = Counter()\n",
    "    train_splited_np = train_df[\"title_desc\"].str.split().values\n",
    "    total_word_count = Counter()\n",
    "    for word_list in train_splited_np: \n",
    "        #total_word_count += Counter(train_splited_np[idx])\n",
    "        total_word_count.update(word_list)\n",
    "    # num_wordsの上限を設ける\n",
    "    num_words = min(max_num_words , len(total_word_count.keys()))\n",
    "\n",
    "    print(\"tokenizerモデルを学習しています\")\n",
    "    train_np = train_df[\"title_desc\"].values\n",
    "    tokenizer = text.Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(list(train_np))\n",
    "\n",
    "    print(\"学習済みtokenizerを保存しています\")\n",
    "    with open(TOKENIZER_PATH, 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"コメントをtokenizeしています\")\n",
    "    tokened_train_list = tokenizer.texts_to_sequences(train_np)\n",
    "    X_train_np = sequence.pad_sequences(tokened_train_list, maxlen=seq_maxlen)\n",
    "    y_train_np = train_df[\"deal_probability\"].values\n",
    "        \n",
    "    return X_train_np, y_train_np, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_train_wvector_coeff(train_df, tokenizer, embed_size=300, seq_maxlen=300):\n",
    "    \"\"\"\n",
    "    トークン化した各単語に対応する学習済みfastTextの重みを抽出。 \n",
    "    また、学習済みfastText, GloVeに存在しない単語をnew_words_listとして抽出し、\n",
    "    train_df中にどのくらい新語が含まれているか計算する。\n",
    "    \"\"\"\n",
    "    print(\"tokenの重みを取得しています\")\n",
    "    ftext_neo_wmatrix, new_words_list_neo = _get_weighted_matrix(tokenizer, \n",
    "                                                                 FTEXT_PRETRAINED_NEOLOGD_PATH, \n",
    "                                                                 embed_size=embed_size,\n",
    "                                                                 seq_maxlen=seq_maxlen)\n",
    "    ftext_wiki_wmatrix, new_words_list_wiki  = _get_weighted_matrix(tokenizer, \n",
    "                                                                    FTEXT_PRETRAINED_WIKI_PATH, \n",
    "                                                                    embed_size=embed_size,\n",
    "                                                                    seq_maxlen=seq_maxlen)\n",
    "    ftext_cc_wmatrix, new_words_list_cc  = _get_weighted_matrix(tokenizer, \n",
    "                                                                FTEXT_PRETRAINED_COMMONCRAWL_PATH, \n",
    "                                                                embed_size=embed_size,\n",
    "                                                                seq_maxlen=seq_maxlen)\n",
    "    # Concatnate 3 new words lists\n",
    "    # listはin演算でsetに対して10^3のオーダー倍遅い\n",
    "    # uniqueの計算時にin演算を使用するのでsetに変換\n",
    "    # set, dict > numpy.array > list の順に早い\n",
    "    new_word_set = set(new_words_list_neo + new_words_list_wiki + new_words_list_cc)\n",
    "\n",
    "    print(\"学習済みword vectorに含まれない新出語の比率を計算しています\")\n",
    "    #train_df[\"word_list\"] = train_df[\"title_desc\"].apply(lambda comment: comment.split())\n",
    "    train_df[\"word_list\"] = train_df[\"title_desc\"].str.split()\n",
    "    # word_list中にnew_word_listの単語がどのくらい含まれているか数える\n",
    "    \n",
    "    unique_rate_np = train_df[\"word_list\"].map(lambda word_list: \n",
    "                                               len([1 for word in word_list if word in new_word_set])\n",
    "                                               / len(word_list) \n",
    "                                               if len(word_list) !=0 else 1).astype(\"float16\").values\n",
    "\n",
    "    return ftext_neo_wmatrix, ftext_wiki_wmatrix, ftext_cc_wmatrix, unique_rate_np.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def uq_rate(word_list):\n",
    "    return len([word for word in word_list if word in new_words_list]) / len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_weighted_matrix(tokenizer, pretrained_path, embed_size=300, seq_maxlen=300):\n",
    "    \"\"\"学習済み単語ベクトルを読み込んで、特徴の重みを計算\"\"\"\n",
    "    # word_and_vector.strip().split() ではうまくいかなかった（ロシア語の場合）\n",
    "    embed_idx = dict(_get_coefs(*word_and_vector.rstrip().rsplit(' ')) for word_and_vector in open(pretrained_path, encoding=\"utf-8\"))\n",
    "    # 次元の不揃いなベクトルをseq_maxlenにpaddingする\n",
    "    embed_idx_val = sequence.pad_sequences(embed_idx.values(), maxlen=seq_maxlen)\n",
    "    all_embs = np.stack(embed_idx_val) # embed_idx_val\n",
    "    embed_mean, embed_std =  all_embs.mean(), all_embs.std()\n",
    "    word_idx = tokenizer.word_index\n",
    "    nb_words = min(tokenizer.num_words, len(word_idx))\n",
    "    embed_matrix = np.random.normal(embed_mean, embed_std, (nb_words, embed_size))\n",
    "    new_words_list = []\n",
    "    for word, idx in word_idx.items():\n",
    "        if idx >= nb_words: continue\n",
    "        embed_vector = embed_idx.get(word)\n",
    "        if embed_vector is not None: embed_matrix[idx] = embed_vector\n",
    "        else: new_words_list.append(word)\n",
    "\n",
    "    return embed_matrix, new_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_coefs(word, *arr):\n",
    "    \"\"\"\n",
    "    入力された*arr(str)にワードベクトルの数値以外の不純物が含まれていることがあるので，\n",
    "    try: 数値データであればarr_fixedにappendする\n",
    "    except: 数値以外の不純物が混じっている場合（floatへキャスト時にValueErrorする場合）は含めない\n",
    "    \"\"\"\n",
    "    arr_fixed = []\n",
    "    for arr_val in arr:\n",
    "        try:\n",
    "            arr_fixed.append(float(arr_val))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    return word, np.asarray(arr_fixed, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_input_dict(input_np, seq_maxlen=300):\n",
    "    \"\"\"2種類の学習済み分散表現モデル（fastText_NEologd, fastText_Wikipedia, fastText_CommonCrawl）で重み付けできるようにdictを作成\"\"\"\n",
    "    input_dict = {\n",
    "                  'ftext_neo': input_np[:, :seq_maxlen],\n",
    "                  'ftext_wiki': input_np[:, :seq_maxlen],\n",
    "                  'ftext_cc': input_np[:, :seq_maxlen],\n",
    "                  'uniq_rate': input_np[:, seq_maxlen]\n",
    "                 }\n",
    "\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_model(len_train, ftext_neo_weight, ftext_wiki_weight, ftext_cc_weight, num_words, seq_maxlen=300, \n",
    "               embed_size=300, batch_size=5000, epochs=2):\n",
    "    inp_ftext_neo = Input(shape=(seq_maxlen, ), name='ftext_neo')\n",
    "    inp_ftext_wiki = Input(shape=(seq_maxlen, ), name='ftext_wiki')\n",
    "    inp_ftext_cc = Input(shape=(seq_maxlen, ), name='ftext_cc')\n",
    "    inp_urate = Input(shape=[1], name='uniq_rate')\n",
    "    embed_ftext_neo = Embedding(num_words, embed_size, weights=[ftext_neo_weight])(inp_ftext_neo)\n",
    "    embed_ftext_wiki = Embedding(num_words, embed_size, weights=[ftext_wiki_weight])(inp_ftext_wiki)\n",
    "    embed_ftext_cc = Embedding(num_words, embed_size, weights=[ftext_cc_weight])(inp_ftext_cc)\n",
    "    conc_embed = concatenate([embed_ftext_neo, embed_ftext_wiki, embed_ftext_cc])\n",
    "    dout = SpatialDropout1D(0.5)(conc_embed)\n",
    "\n",
    "    lstmed = Bidirectional(CuDNNLSTM(40, return_sequences=True, go_backwards=True))(dout)\n",
    "    grued = Bidirectional(CuDNNGRU(40, return_sequences=True, go_backwards=True))(lstmed)\n",
    "    avg_pooled = GlobalAveragePooling1D()(grued)\n",
    "    max_pooled = GlobalMaxPooling1D()(grued)\n",
    "    conc_pool_urate = concatenate([avg_pooled, max_pooled, inp_urate])\n",
    "    outs = Dense(1, activation=\"linear\")(conc_pool_urate) # linear, sigmoid\n",
    "\n",
    "    inputs = [inp_ftext_neo, inp_ftext_wiki, inp_ftext_cc, inp_urate]\n",
    "    model = Model(inputs=inputs, outputs=outs)\n",
    "\n",
    "    # 重み減数の設定\n",
    "    steps = int(len_train / batch_size) * epochs\n",
    "    lr_init, lr_fin = 0.002, 0.0002\n",
    "    optimizer_adam = Adam(lr=0.002, decay=_get_exp_decay(lr_init, lr_fin, steps))\n",
    "\n",
    "    model.compile(loss=\"mean_squared_error\", # root_mean_squared_error\n",
    "                  optimizer=optimizer_adam,\n",
    "                  metrics=[root_mean_squared_error]) # , \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_exp_decay(init, fin, steps): \n",
    "    return (init / fin) ** (1 / (steps - 1)) - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test合同でtokenizeするとなぜかval_loss が発散するバグある．\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_test_feature(test_data, seq_maxlen=300):\n",
    "    test_np = test_data[\"title_desc\"].values\n",
    "    if os.path.exists(TOKENIZER_PATH):\n",
    "        print(\"学習済みtokenizerをロードしています\")\n",
    "        with open(TOKENIZER_PATH, 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "    else:\n",
    "        sys.exit(\"学習済みtokenizerが見つかりません\")\n",
    "    \n",
    "    tokened_test_list = tokenizer.texts_to_sequences(test_np)\n",
    "    X_test_np = sequence.pad_sequences(tokened_test_list, maxlen=seq_maxlen)\n",
    "        \n",
    "    return X_test_np, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_test_wvector_coeff(test_df, tokenizer, embed_size=300, seq_maxlen=300):\n",
    "    new_words_set = set([])\n",
    "    if os.path.exists(NEW_WORDS_LIST_PATH):\n",
    "        with open(NEW_WORDS_LIST_PATH, 'rb') as handle:\n",
    "            new_words_list = pickle.load(handle)\n",
    "    else:\n",
    "        print(\"new words listが存在しないので、作成します\")\n",
    "        _, new_words_list_neo = _get_weighted_matrix(tokenizer, \n",
    "                                                     FTEXT_PRETRAINED_NEOLOGD_PATH, \n",
    "                                                     embed_size=embed_size,\n",
    "                                                     seq_maxlen=seq_maxlen)\n",
    "        _, new_words_list_wiki = _get_weighted_matrix(tokenizer, \n",
    "                                                      FTEXT_PRETRAINED_WIKI_PATH, \n",
    "                                                      embed_size=embed_size,\n",
    "                                                      seq_maxlen=seq_maxlen)\n",
    "        _, new_words_list_cc = _get_weighted_matrix(tokenizer, \n",
    "                                                    FTEXT_PRETRAINED_COMMONCRAWL_PATH, \n",
    "                                                    embed_size=embed_size,\n",
    "                                                    seq_maxlen=seq_maxlen)\n",
    "        new_words_set = set(new_words_list_neo + new_words_list_wiki + new_words_list_cc)\n",
    "    test_df[\"word_list\"] = test_df[\"title_desc\"].str.split()\n",
    "    unique_rate_np = test_df[\"word_list\"].map(lambda word_list: \n",
    "                                              len([1 for word in word_list if word in new_words_set]) \n",
    "                                              / len(word_list) \n",
    "                                              if len(word_list) !=0 else 1).astype(\"float16\").values\n",
    "\n",
    "    return unique_rate_np.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "\tt0 = time.time()\n",
    "\tyield\n",
    "\tprint(f'[{name}] done in {time.time() - t0:.0f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "#from keras.models = import Model \n",
    "\n",
    "class ModelSave(Callback):\n",
    "    def __init__(self, interval=1, fold_idx=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.fold_idx = fold_idx\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            print(\"model save\")\n",
    "            if self.fold_idx > 0:\n",
    "                self.model.save(\"best_model_{:d}f_ep{:d}.h5\".format(self.fold_idx, epoch+1))\n",
    "            else:\n",
    "                self.model.save(\"best_model_no_cross_val_ep{:d}.h5\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process (Cross Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.read_csv(\"text_train_test.csv\",\n",
    "                            dtype={\"item_id\":str, \"title\":str, \"description\":str, \"deal_probability\":float})\n",
    "\n",
    "train_test_df[\"title_desc\"] = train_test_df[\"title\"] + \" \" + train_test_df[\"description\"]\n",
    "train_test_df[\"title_desc\"] = train_test_df[\"title_desc\"].fillna(\"без описания\") \n",
    "train_test_df[\"title_desc\"] = train_test_df[\"title_desc\"].apply(lambda text: str(text) \\\n",
    "                                                                if type(text) != str else text) \n",
    "train_test_df[\"title_desc\"] = train_test_df[\"title_desc\"].apply(lambda text: text.lower())\n",
    "train_test_df[\"title_desc\"] = train_test_df[\"title_desc\"].apply(lambda text: cleanName(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100"
     ]
    }
   ],
   "source": [
    "# Update 2018-06-24\n",
    "# ストップワード除去\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#russian_stop = set(stopwords.words('russian'))\n",
    "#train_test_df[\"title_desc\"] = train_test_df[\"title_desc\"].apply(lambda text: remove_stopwords(text, russian_stop))\n",
    "# 特殊データの除去\n",
    "elim_id_list = pd.read_csv(\"elim_item_id.csv\")\n",
    "import sys\n",
    "elim_idx_list = []\n",
    "for idx, item in enumerate(elim_id_list[\"item_id\"]):\n",
    "    elim_idx = train_test_df[train_test_df[\"item_id\"] == item].index\n",
    "    if len(elim_idx) != 0:\n",
    "        elim_idx_list.append(list(elim_idx)[0])\n",
    "    sys.stdout.write(\"\\r {:d}\".format(idx+1))\n",
    "    sys.stdout.flush()\n",
    "train_test_df = train_test_df.drop(train_test_df.index[elim_idx_list]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_df = train_test_df[~train_test_df[\"deal_probability\"].isnull()]\n",
    "test_df = train_test_df[train_test_df[\"deal_probability\"].isnull()]\n",
    "train_df = train_df.reset_index()\n",
    "test_df = test_df.reset_index()\n",
    "\n",
    "len_train = len(train_df)\n",
    "len_test = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test remerge\n",
    "train_test_df = train_df.append(test_df)\n",
    "train_test_df = train_test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データの語彙数を計算しています\n",
      "tokenizerモデルを学習しています\n",
      "学習済みtokenizerを保存しています\n",
      "コメントをtokenizeしています\n",
      "tokenの重みを取得しています\n",
      "学習済みword vectorに含まれない新出語の比率を計算しています\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "WARNING:tensorflow:From C:\\Users\\akio\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1438s 1ms/step - loss: 0.0544 - root_mean_squared_error: 0.1585 - val_loss: 0.0527 - val_root_mean_squared_error: 0.1561\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1429s 1ms/step - loss: 0.0506 - root_mean_squared_error: 0.1504 - val_loss: 0.0526 - val_root_mean_squared_error: 0.1541\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_1f_seed7.pklとして履歴を保存しています。\n",
      "seed=7のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 103s 273us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 139s 273us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1431s 1ms/step - loss: 0.0543 - root_mean_squared_error: 0.1582 - val_loss: 0.0526 - val_root_mean_squared_error: 0.1531\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1429s 1ms/step - loss: 0.0506 - root_mean_squared_error: 0.1503 - val_loss: 0.0527 - val_root_mean_squared_error: 0.1518\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_2f_seed7.pklとして履歴を保存しています。\n",
      "seed=7のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 103s 274us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 139s 274us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1431s 1ms/step - loss: 0.0544 - root_mean_squared_error: 0.1584 - val_loss: 0.0527 - val_root_mean_squared_error: 0.1583\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1431s 1ms/step - loss: 0.0507 - root_mean_squared_error: 0.1504 - val_loss: 0.0527 - val_root_mean_squared_error: 0.1551\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_3f_seed7.pklとして履歴を保存しています。\n",
      "seed=7のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 103s 275us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 140s 276us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1455s 1ms/step - loss: 0.0544 - root_mean_squared_error: 0.1585 - val_loss: 0.0524 - val_root_mean_squared_error: 0.1570\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1447s 1ms/step - loss: 0.0507 - root_mean_squared_error: 0.1504 - val_loss: 0.0524 - val_root_mean_squared_error: 0.1521\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_4f_seed7.pklとして履歴を保存しています。\n",
      "seed=7のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 104s 276us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 139s 273us/step\n",
      "[Cross validation elapsed time:] done in 12588 s\n",
      "Save out-of-fold X, y train array\n",
      "writing...\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1448s 1ms/step - loss: 0.0544 - root_mean_squared_error: 0.1584 - val_loss: 0.0535 - val_root_mean_squared_error: 0.1433\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1445s 1ms/step - loss: 0.0506 - root_mean_squared_error: 0.1504 - val_loss: 0.0528 - val_root_mean_squared_error: 0.1544\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_1f_seed42.pklとして履歴を保存しています。\n",
      "seed=42のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 105s 280us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 141s 278us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1451s 1ms/step - loss: 0.0543 - root_mean_squared_error: 0.1583 - val_loss: 0.0528 - val_root_mean_squared_error: 0.1456\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1446s 1ms/step - loss: 0.0506 - root_mean_squared_error: 0.1503 - val_loss: 0.0524 - val_root_mean_squared_error: 0.1521\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_2f_seed42.pklとして履歴を保存しています。\n",
      "seed=42のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 103s 274us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 143s 281us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1443s 1ms/step - loss: 0.0545 - root_mean_squared_error: 0.1587 - val_loss: 0.0525 - val_root_mean_squared_error: 0.1540\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1449s 1ms/step - loss: 0.0508 - root_mean_squared_error: 0.1506 - val_loss: 0.0525 - val_root_mean_squared_error: 0.1572\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_3f_seed42.pklとして履歴を保存しています。\n",
      "seed=42のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 104s 277us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 143s 281us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1449s 1ms/step - loss: 0.0543 - root_mean_squared_error: 0.1582 - val_loss: 0.0529 - val_root_mean_squared_error: 0.1577\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1443s 1ms/step - loss: 0.0506 - root_mean_squared_error: 0.1502 - val_loss: 0.0528 - val_root_mean_squared_error: 0.1497\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_4f_seed42.pklとして履歴を保存しています。\n",
      "seed=42のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 102s 273us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 145s 284us/step\n",
      "[Cross validation elapsed time:] done in 12711 s\n",
      "Save out-of-fold X, y train array\n",
      "writing...\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1450s 1ms/step - loss: 0.0545 - root_mean_squared_error: 0.1586 - val_loss: 0.0525 - val_root_mean_squared_error: 0.1500\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1436s 1ms/step - loss: 0.0507 - root_mean_squared_error: 0.1504 - val_loss: 0.0526 - val_root_mean_squared_error: 0.1521\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_1f_seed10.pklとして履歴を保存しています。\n",
      "seed=10のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 102s 272us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 139s 274us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1418s 1ms/step - loss: 0.0544 - root_mean_squared_error: 0.1585 - val_loss: 0.0529 - val_root_mean_squared_error: 0.1496\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1413s 1ms/step - loss: 0.0506 - root_mean_squared_error: 0.1503 - val_loss: 0.0527 - val_root_mean_squared_error: 0.1487\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_2f_seed10.pklとして履歴を保存しています。\n",
      "seed=10のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 101s 270us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 137s 269us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1413s 1ms/step - loss: 0.0544 - root_mean_squared_error: 0.1585 - val_loss: 0.0527 - val_root_mean_squared_error: 0.1534\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1428s 1ms/step - loss: 0.0506 - root_mean_squared_error: 0.1503 - val_loss: 0.0526 - val_root_mean_squared_error: 0.1526\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_3f_seed10.pklとして履歴を保存しています。\n",
      "seed=10のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 103s 273us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 139s 273us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1437s 1ms/step - loss: 0.0543 - root_mean_squared_error: 0.1582 - val_loss: 0.0526 - val_root_mean_squared_error: 0.1537\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1444s 1ms/step - loss: 0.0505 - root_mean_squared_error: 0.1500 - val_loss: 0.0529 - val_root_mean_squared_error: 0.1488\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_4f_seed10.pklとして履歴を保存しています。\n",
      "seed=10のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 102s 271us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 137s 270us/step\n",
      "[Cross validation elapsed time:] done in 12525 s\n",
      "Save out-of-fold X, y train array\n",
      "writing...\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1422s 1ms/step - loss: 0.0544 - root_mean_squared_error: 0.1585 - val_loss: 0.0527 - val_root_mean_squared_error: 0.1544\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1423s 1ms/step - loss: 0.0506 - root_mean_squared_error: 0.1502 - val_loss: 0.0527 - val_root_mean_squared_error: 0.1560\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_1f_seed33.pklとして履歴を保存しています。\n",
      "seed=33のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 102s 271us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 138s 272us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1424s 1ms/step - loss: 0.0543 - root_mean_squared_error: 0.1582 - val_loss: 0.0529 - val_root_mean_squared_error: 0.1477\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1423s 1ms/step - loss: 0.0506 - root_mean_squared_error: 0.1501 - val_loss: 0.0527 - val_root_mean_squared_error: 0.1501\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_2f_seed33.pklとして履歴を保存しています。\n",
      "seed=33のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 102s 271us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 138s 270us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1423s 1ms/step - loss: 0.0543 - root_mean_squared_error: 0.1583 - val_loss: 0.0524 - val_root_mean_squared_error: 0.1528\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1423s 1ms/step - loss: 0.0507 - root_mean_squared_error: 0.1504 - val_loss: 0.0524 - val_root_mean_squared_error: 0.1543\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_3f_seed33.pklとして履歴を保存しています。\n",
      "seed=33のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 102s 271us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 138s 272us/step\n",
      "train size:  1127493\n",
      "valid size:  375831\n",
      "test size :  508438\n",
      "Train on 1127493 samples, validate on 375831 samples\n",
      "Epoch 1/2\n",
      "1127493/1127493 [==============================] - 1423s 1ms/step - loss: 0.0545 - root_mean_squared_error: 0.1587 - val_loss: 0.0529 - val_root_mean_squared_error: 0.1505\n",
      "model save\n",
      "Epoch 2/2\n",
      "1127493/1127493 [==============================] - 1422s 1ms/step - loss: 0.0508 - root_mean_squared_error: 0.1507 - val_loss: 0.0526 - val_root_mean_squared_error: 0.1508\n",
      "model save\n",
      "学習が完了しました。ファイル名：history_4f_seed33.pklとして履歴を保存しています。\n",
      "seed=33のoof_trainを取得します．\n",
      "375831/375831 [==============================] - 102s 272us/step\n",
      "Predicting...\n",
      "508438/508438 [==============================] - 139s 272us/step\n",
      "[Cross validation elapsed time:] done in 12445 s\n",
      "Save out-of-fold X, y train array\n",
      "writing...\n"
     ]
    }
   ],
   "source": [
    "X_train_test_np, y_train_test_np, tokenizer = _get_train_feature(train_test_df, seq_maxlen=seq_maxlen)\n",
    "ftext_neo_wmatrix, ftext_wiki_wmatrix, ftext_cc_wmatrix, unique_rate_np = _get_train_wvector_coeff(\n",
    "    train_test_df, \n",
    "    tokenizer, \n",
    "    embed_size=embed_size, \n",
    "    seq_maxlen=seq_maxlen)\n",
    "X_train_test_np = np.concatenate([X_train_test_np, unique_rate_np], axis=1)\n",
    "gc.collect()\n",
    "\n",
    "val_ratio = 0.05\n",
    "batch_size = 250\n",
    "epochs = 2\n",
    "num_words = min(tokenizer.num_words, len(tokenizer.word_index))\n",
    "\n",
    "X_train_np = X_train_test_np[:len_train]\n",
    "y_train_np = y_train_test_np[:len_train]\n",
    "X_test_np = X_train_test_np[len_train:]\n",
    "\n",
    "\"\"\"Update 2018-06-24 seed averager\"\"\"\n",
    "seeds = [7, 42, 10, 33]\n",
    "for seed in seeds:\n",
    "    kfold = KFold(n_splits=fold_num, shuffle=True, random_state=seed)\n",
    "    # Initialize oof_xtrain\n",
    "    oof_X_train = []\n",
    "    oof_y_train = []\n",
    "    # initialize output\n",
    "    sum_output =[]\n",
    "\n",
    "    with timer(\"Cross validation elapsed time:\"):\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_train_np)):\n",
    "            X_val_np_kf = X_train_np[val_idx]\n",
    "            X_train_np_kf =X_train_np[train_idx]\n",
    "            y_val_np_kf = y_train_np[val_idx] \n",
    "            y_train_np_kf = y_train_np[train_idx]\n",
    "\n",
    "            print(\"train size: \", len(X_train_np_kf))\n",
    "            print(\"valid size: \", len(X_val_np_kf))\n",
    "            print(\"test size : \", len(X_test_np))\n",
    "            gc.collect()\n",
    "\n",
    "            len_train = len(X_train_np_kf)\n",
    "\n",
    "            X_train_dict = _get_input_dict(X_train_np_kf, seq_maxlen)\n",
    "            X_val_dict = _get_input_dict(X_val_np_kf, seq_maxlen)\n",
    "            del X_train_np_kf, X_val_np_kf\n",
    "            gc.collect()\n",
    "\n",
    "            model = _get_model(len_train, \n",
    "                               ftext_neo_wmatrix, \n",
    "                               ftext_wiki_wmatrix,\n",
    "                               ftext_cc_wmatrix,\n",
    "                               num_words=num_words,  \n",
    "                               seq_maxlen=seq_maxlen,\n",
    "                               embed_size=embed_size,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs)\n",
    "\n",
    "\n",
    "            modelsaver = ModelSave(fold_idx=fold_idx+1)\n",
    "\n",
    "            history = model.fit(X_train_dict, \n",
    "                                y_train_np_kf, \n",
    "                                batch_size=batch_size, \n",
    "                                epochs=epochs, \n",
    "                                validation_data=(X_val_dict, y_val_np_kf),\n",
    "                                callbacks=[modelsaver],\n",
    "                                #class_weight={0:0.01, 1:.99}, \n",
    "                                shuffle=True, \n",
    "                                verbose=1)\n",
    "\n",
    "            MODEL_HISTORY_PATH = \"history_{:d}f_seed{:d}.pkl\".format(fold_idx+1, seed)\n",
    "            print(\"学習が完了しました。ファイル名：{:s}として履歴を保存しています。\".format(MODEL_HISTORY_PATH))\n",
    "            with open(MODEL_HISTORY_PATH, 'wb') as handle:\n",
    "                pickle.dump(history.history, handle)\n",
    "\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"seed={:d}のoof_trainを取得します．\".format(seed))       \n",
    "            best_epoch = np.argmin(history.history[\"val_loss\"]) + 1\n",
    "            best_model = load_model(\"best_model_{:d}f_ep{:d}.h5\".format(fold_idx+1, best_epoch), \n",
    "                                    custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
    "            oof_X_train_tmp = best_model.predict(X_val_dict, batch_size=batch_size, verbose=1)\n",
    "            # concat oof_train\n",
    "            if fold_idx == 0:\n",
    "                oof_X_train = oof_X_train_tmp\n",
    "                oof_y_train = y_val_np_kf\n",
    "            else:\n",
    "                oof_X_train = np.concatenate([oof_X_train, oof_X_train_tmp], axis=0)\n",
    "                oof_y_train = np.concatenate([oof_y_train, y_val_np_kf], axis=0)        \n",
    "\n",
    "            print(\"Predicting...\")\n",
    "            X_test_dict = _get_input_dict(X_test_np, seq_maxlen)\n",
    "            if fold_idx == 0:\n",
    "                sum_output = best_model.predict(X_test_dict, batch_size=batch_size, verbose=1)\n",
    "            else:\n",
    "                sum_output += best_model.predict(X_test_dict, batch_size=batch_size, verbose=1)\n",
    "\n",
    "            K.clear_session()\n",
    "            del best_model\n",
    "            gc.collect()\n",
    "\n",
    "    print('Save out-of-fold X, y train array')\n",
    "    np.save('bi_lstm_gru_ftext_neo_wiki_cc_oof_X_train_{:d}f_seed{:d}.npy'.format(fold_num, seed),\n",
    "            oof_X_train)\n",
    "    np.save('bi_lstm_gru_ftext_neo_wiki_cc_oof_y_train_{:d}f_seed{:d}.npy'.format(fold_num, seed),\n",
    "            oof_y_train)\n",
    "\n",
    "    print(\"writing...\")\n",
    "    outputs = sum_output / fold_num\n",
    "    test_df[\"deal_probability\"] = outputs\n",
    "    sub_df = pd.read_csv(\"sample_submission.csv\")\n",
    "    sub_df = pd.merge(sub_df[[\"item_id\"]], test_df[[\"item_id\", \"deal_probability\"]], how='left')\n",
    "    sub_df[\"deal_probability\"] = sub_df[\"deal_probability\"].apply(lambda x: 0. if x < 0 else x)\n",
    "    sub_df[\"deal_probability\"] = sub_df[\"deal_probability\"].apply(lambda x: 1. if x > 1 else x)\n",
    "    sub_df[[\"item_id\",\"deal_probability\"]].to_csv( \\\n",
    "        'bi_lstm_gru_ftext_neo_wiki_cc_{:d}f_seed{:d}.csv'.format(fold_num, seed), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed averager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_proba = None\n",
    "for idx, seed in enumerate(seeds):\n",
    "    sub_df = pd.read_csv('bi_lstm_gru_ftext_neo_wiki_cc_{:d}f_seed{:d}.csv'\n",
    "                         .format(fold_num, seed))\n",
    "    if idx == 0:\n",
    "        tmp_proba = sub_df[\"deal_probability\"].values\n",
    "    else:\n",
    "        tmp_proba +=  sub_df[\"deal_probability\"].values\n",
    "outputs = tmp_proba / len(seeds)\n",
    "sub_df[\"deal_probability\"] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"bi_lstm_gru_seed_avg_7_42_10_33.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
